{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "import cuml\n",
    "\n",
    "import cupy as cp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches\n",
    "\n",
    "import os\n",
    "import warnings\n",
    "import tqdm.notebook as tqdm\n",
    "\n",
    "transformers.logging.set_verbosity_error()\n",
    "warnings.filterwarnings(\"ignore\", message=\"Unable to import Triton*\")\n",
    "\n",
    "# path\n",
    "data_fold_cpu = \"/mnt/efs_v2/tag_onc/users/tianrui.qi/TCGA-Onc/data/public/\"\n",
    "data_fold_gpu = \"/mnt/efs_v2/dbgap_tcga/users/tianrui.qi/TCGA-Onc/data/public/\"\n",
    "csv_load_fold_cpu = os.path.join(data_fold_cpu, \"csv\")\n",
    "csv_load_fold_gpu = os.path.join(data_fold_gpu, \"csv\")\n",
    "\n",
    "# dataset\n",
    "profile = pd.read_csv(os.path.join(data_fold_cpu, \"profile.txt\"))\n",
    "# Isolate string (i.e. su001) -> int (i.e. 1)\n",
    "profile[\"Isolate\"] = profile[\"Isolate\"].apply(lambda x: int(x[2:]))\n",
    "# Treatment pre/post -> 0/1\n",
    "profile[\"Treatment\"] = profile[\"Treatment\"].apply(lambda x: int(not \"pre\" in x))\n",
    "\n",
    "# parameter\n",
    "chr = \"1\"\n",
    "pval_thresh = 1e-4\n",
    "batch_size = int(1e2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model for embedding\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "    \"zhihan1996/DNABERT-2-117M\", trust_remote_code=True\n",
    ")\n",
    "dnabert2 = transformers.AutoModel.from_pretrained(\n",
    "    \"zhihan1996/DNABERT-2-117M\", trust_remote_code=True\n",
    ").to(device).eval()\n",
    "\n",
    "chr_list = [str(i) for i in range(1, 23)] + [\"X\"]   # BAM naming convention\n",
    "\n",
    "for run in tqdm.tqdm(profile[\"Run\"], smoothing=0.0, unit=\"run\"):\n",
    "    # load the csv, try gpu first then cpu\n",
    "    if os.path.exists(os.path.join(csv_load_fold_gpu, f\"{run}/{chr}.csv\")):\n",
    "        csv = pd.read_csv(os.path.join(csv_load_fold_gpu, f\"{run}/{chr}.csv\"))\n",
    "    else:\n",
    "        csv = pd.read_csv(os.path.join(csv_load_fold_cpu, f\"{run}/{chr}.csv\"))\n",
    "    if \"embedding\" not in csv.columns:\n",
    "        csv[\"embedding\"] = None\n",
    "\n",
    "    # get the index of reads that pass the pval_thresh and embedding is None\n",
    "    temp = csv\n",
    "    temp = temp[temp[str(pval_thresh)]>=1]\n",
    "    temp = temp[pd.isnull(temp[\"embedding\"])]\n",
    "    index = temp.index\n",
    "\n",
    "    # all embedding is calculated, skip\n",
    "    if len(index) == 0:\n",
    "        continue\n",
    "\n",
    "    # calculate the embedding\n",
    "    for i in tqdm.tqdm(\n",
    "        range(int(np.ceil(len(index) / batch_size))), \n",
    "        smoothing=0.0, desc=run, unit=\"batch\", leave=False\n",
    "    ):  \n",
    "        # sequence\n",
    "        index_batch = index[i*batch_size:(i+1)*batch_size]\n",
    "        sequence_batch = csv[\"sequence\"].loc[index_batch].to_list()\n",
    "        # token\n",
    "        token_batch = tokenizer(\n",
    "            sequence_batch, return_tensors = 'pt', padding=True\n",
    "        )[\"input_ids\"].to(device)\n",
    "        # embedding\n",
    "        with torch.no_grad(): \n",
    "            embedding_batch = torch.mean(\n",
    "                dnabert2(token_batch)[0], dim=1\n",
    "            ).detach().cpu().numpy()\n",
    "        # update csv dataframe\n",
    "        for i in range(len(index_batch)):\n",
    "            csv.at[index_batch[i], \"embedding\"] = embedding_batch[i]\n",
    "\n",
    "    # save to gpu fold so that we can skip the calculation next time\n",
    "    if not os.path.exists(os.path.join(csv_load_fold_gpu, f\"{run}\")): \n",
    "        os.makedirs(os.path.join(csv_load_fold_gpu, f\"{run}\"))\n",
    "    csv.to_csv(os.path.join(csv_load_fold_gpu, f\"{run}/{chr}.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dict = {}\n",
    "for run in tqdm.tqdm(profile[\"Run\"], smoothing=0.0, unit=\"run\"):\n",
    "    csv = pd.read_csv(os.path.join(csv_load_fold_gpu, f\"{run}/{chr}.csv\"))\n",
    "    index = csv[csv[str(pval_thresh)]>=1].index\n",
    "    embedding_dict[run] = csv[\"embedding\"].loc[index].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = cp.array(\n",
    "    np.concatenate([embedding_dict[run] for run in profile[\"Run\"]], axis=0)\n",
    ").astype(cp.float32)\n",
    "embedding = cp.asnumpy(\n",
    "    cuml.UMAP(n_components=2, verbose=True).fit_transform(embedding)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color = np.concatenate([\n",
    "    np.ones(len(embedding_dict[profile[\"run\"][i]])) * profile[\"Treatment\"][i]%2 \n",
    "    for i in range(len(profile))\n",
    "], axis=0).reshape(-1, 1)\n",
    "b_patch = matplotlib.patches.Patch(color=\"r\", label=\"pre \")\n",
    "r_patch = matplotlib.patches.Patch(color=\"b\", label=\"post\")\n",
    "\n",
    "# concate embedding and color to shuffle\n",
    "x = np.concatenate([embedding, color], axis=1)\n",
    "x = x[np.random.permutation(np.arange(len(x))), :]\n",
    "\n",
    "color = np.vectorize(lambda x: {0: \"r\", 1: \"b\"}[x])(x[:, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(dpi=1000, figsize=(8, 8))\n",
    "plt.scatter(\n",
    "    x[:, 0], x[:, 1], c=color,\n",
    "    s=0.0001, marker=\"o\"\n",
    ")\n",
    "\n",
    "plt.xlabel(\"UMAP1\")\n",
    "plt.ylabel(\"UMAP2\")\n",
    "plt.title(f\"chromosome {chr}; pval={pval_thresh}\")\n",
    "plt.legend(handles=[b_patch, r_patch])\n",
    "plt.show()\n",
    "\n",
    "plt.figure(dpi=1000, figsize=(8, 8))\n",
    "plt.scatter(\n",
    "    x[color==\"r\"][:, 0], x[color==\"r\"][:, 1], c=\"r\",\n",
    "    s=0.0001, marker=\"o\"\n",
    ")\n",
    "plt.xlabel(\"UMAP1\")\n",
    "plt.ylabel(\"UMAP2\")\n",
    "plt.title(f\"chromosome {chr}; pval={pval_thresh}\")\n",
    "plt.legend(handles=[b_patch, r_patch])\n",
    "plt.show()\n",
    "\n",
    "plt.figure(dpi=1000, figsize=(8, 8))\n",
    "plt.scatter(\n",
    "    x[color==\"b\"][:, 0], x[color==\"b\"][:, 1], c=\"b\",\n",
    "    s=0.0001, marker=\"o\"\n",
    ")\n",
    "plt.xlabel(\"UMAP1\")\n",
    "plt.ylabel(\"UMAP2\")\n",
    "plt.title(f\"chromosome {chr}; pval={pval_thresh}\")\n",
    "plt.legend(handles=[b_patch, r_patch])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TCGA-Onc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
